{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2eeb33-50be-4d00-a1c2-e03ad6c76349",
   "metadata": {},
   "source": [
    "# Intentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ca75c6-53b3-49d6-98f5-859cea5cb706",
   "metadata": {},
   "source": [
    "DA-VINCIS: is the acronym for **Detection of Aggressive and Violent Incidents from Social Media in Spanish**\n",
    "\n",
    "> With this project we try to detect the aggresive and violent content, with this work we are trying to be able to prevent identify people that witness or experience violence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc497d-f84f-48c1-9d09-dd7a510e7890",
   "metadata": {},
   "source": [
    "# Imports and Global Variables Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2840522-f51c-4a26-8fa8-f3cca3b5395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar librerías. Esta casilla es últil por ejemplo si se ejecuta el cuaderno en Google Colab\n",
    "# Note que existen otras dependencias como tensorflow, etc. que en este caso se encontrarían ya instaladas\n",
    "#\n",
    "!pip install autogoal[contrib]==0.4.4\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba659ab4-9380-4600-ae7d-46bc0783b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install WordCloud\n",
    "#!pip install NLTK\n",
    "#!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b8208-e9ef-4b81-a3a1-e6c5adfa8673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  para construir gráficas y realizar análisis exploratorio de los datos\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "\n",
    "# para cargar datos y realizar pre-procesamiento básico\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# para evaluar los modelos \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, f1_score\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "# para configurar AutoGOAL\n",
    "from autogoal.ml import AutoML\n",
    "from autogoal.search import (Logger, PESearch, ConsoleLogger, ProgressLogger, MemoryLogger)\n",
    "from autogoal.kb import MatrixContinuousDense, VectorCategorical, Supervised\n",
    "from autogoal.contrib import find_classes\n",
    "\n",
    "# para guardar el modelo\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words_sp = stopwords.words('spanish')\n",
    "\n",
    "#### CARLOS OSMAN SUAZO BACA #####\n",
    "# Cargamos esta librería para revisar el desbalance que existe en el dataset de tripadvisor\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "# Para contar los parámetros de las clases \n",
    "from collections import Counter\n",
    "\n",
    "print(type(stop_words_sp))\n",
    "print(stop_words_sp)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb1ff5-1e9f-419b-89f7-89708dab933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# para construir pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# para evaluar los modelos \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# algoritmos de clasificación\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "TEXT_COL      = 'tweet'\n",
    "CLASS_COL_ST1 = 'Sentiment'\n",
    "CLASS_COL_ST2 = ['Theft', 'Homicide', 'Kidnapping', 'Accident', 'None of the above']\n",
    "\n",
    "# Declaramos algunas variables globales\n",
    "N_JOBS = 6 # Número de núclos a implementar por gridsearch para el hyper parámeter tuning\n",
    "CV = 5 # Número de interaciones para hacer cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90f3d81-b073-4012-98a3-6f9f0233e663",
   "metadata": {},
   "source": [
    "# Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a72ebb2-1054-4253-9f52-77b11e87fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# función auxiliar utilizada por CountVectorizer para procesar las frases\n",
    "def spanish_stemmer(sentence):\n",
    "    stemmer = SpanishStemmer()\n",
    "    analyzer = CountVectorizer(binary=False, analyzer='word', stop_words=stop_words_sp,\n",
    "                               ngram_range=(1, 1)).build_analyzer()\n",
    "    return (stemmer.stem(word) for word in analyzer(sentence))\n",
    "\n",
    "\n",
    "# guarda un pipeline entrenado\n",
    "def save_model(model, modelName = \"models/pickle_model.pkl\"):\n",
    "   pkl_filename = modelName\n",
    "   with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)   \n",
    "\n",
    "\n",
    "# carga un pipeline entrenado y guardado previamente\n",
    "def load_model(rutaModelo = \"models/pickle_model.pkl\"):\n",
    "  # Load from file\n",
    "  with open(rutaModelo, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)\n",
    "    return pickle_model \n",
    "\n",
    "\n",
    "# función auxiliar para realizar predicciones con el modelo\n",
    "def predict_model(model, data, pref='m'):\n",
    "  \"\"\"\n",
    "  data: list of the text to predict\n",
    "  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n",
    "  \"\"\"\n",
    "  res = {}\n",
    "  scores = None\n",
    "  labels = model.predict(data)\n",
    "\n",
    "  if hasattr(model, 'predict_proba'):\n",
    "    scores = model.predict_proba(data)\n",
    "  \n",
    "    # empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n",
    "    #res = {f'scores_{pref}_{cls.lower()}':score for cls, score in zip(model.classes_, [col for col in scores.T])}\n",
    "    res = {f'scores_{pref}_{cls}':score for cls, score in zip(model.classes_, [col for col in scores.T])}\n",
    "\n",
    "  # añadir datos relativos a la predicción\n",
    "  res[f'labels_{pref}'] = labels\n",
    "\n",
    "  # convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabeticamente.\n",
    "  res = pd.DataFrame(res, columns=sorted(list(res.keys())))\n",
    "\n",
    "  return res\n",
    "\n",
    "\n",
    "# función auxiliar que evalúa los resultados de una clasificación\n",
    "def evaluate_model(y_true, y_pred, y_score=None, pos_label='positive'):\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\"\n",
    "  print('==== Sumario de la clasificación ==== ')\n",
    "  print(classification_report(y_true, y_pred))\n",
    "\n",
    "  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "  # graficar matriz de confusión\n",
    "  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n",
    "  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n",
    "\n",
    "  z = cm[::-1]\n",
    "  x = display_labels\n",
    "  y =  x[::-1].copy()\n",
    "  z_text = [[str(y) for y in x] for x in z]\n",
    "\n",
    "  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "  fig_cm.update_layout(\n",
    "      height=400, width=400,\n",
    "      showlegend=True,\n",
    "      margin={'t':150, 'l':0},\n",
    "      title={'text' : 'Matriz de Confusión', 'x':0.5, 'xanchor': 'center'},\n",
    "      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n",
    "      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n",
    "  )\n",
    "  fig_cm.show()\n",
    "\n",
    "\n",
    "# función auxiliar para realizar predicciones con el modelo\n",
    "def _predict_model(model, cfg, data, pref='m'):\n",
    "  \"\"\"\n",
    "  data: list of the text to predict\n",
    "  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n",
    "  \"\"\"\n",
    "  res = {}\n",
    "  scores = None\n",
    "\n",
    "  data_tfidf = cfg['vectorizer'].transform(data)\n",
    "  data_tfidf = data_tfidf.todense()\n",
    "  labels = model.predict(data_tfidf)\n",
    "\n",
    "  if hasattr(model, 'predict_proba'):\n",
    "    scores = model.predict_proba(data_tfidf)\n",
    "  \n",
    "    # empaquetar scores dentro de un diccionario que contiene labels, scores clase 1, scores clase 2, .... El nombre de la clase se normaliza a lowercase\n",
    "    res = {f'scores_{pref}_{cls.lower()}':score for cls, score in zip(model.classes_, [col for col in scores.T])}\n",
    "\n",
    "  # añadir datos relativos a la predicción\n",
    "  res[f'labels_{pref}'] = cfg['label_encoder'].inverse_transform(labels)\n",
    "\n",
    "  # convertir a dataframe ordenando las columnas primero el label y luego los scores por clase, las clases ordenadas alfabeticamente.\n",
    "  res = pd.DataFrame(res, columns=sorted(list(res.keys())))\n",
    "\n",
    "  return res\n",
    "\n",
    "\n",
    "# función auxiliar que evalúa los resultados de una clasificación\n",
    "def _evaluate_model(y_true, y_pred, y_score=None, pos_label='positive'):\n",
    "  \"\"\"\n",
    "  data: list of the text to predict\n",
    "  pref: identificador para las columnas (labels_[pref], scores_[pref]_[class 1], etc.)\n",
    "  \"\"\"\n",
    "  print('==== Sumario de la clasificación ==== ')\n",
    "  print(classification_report(y_true, y_pred))\n",
    "\n",
    "  print('Accuracy -> {:.2%}\\n'.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "  # graficar matriz de confusión\n",
    "  display_labels = sorted(unique_labels(y_true, y_pred), reverse=True)\n",
    "  cm = confusion_matrix(y_true, y_pred, labels=display_labels)\n",
    "\n",
    "  z = cm[::-1]\n",
    "  x = display_labels\n",
    "  y =  x[::-1].copy()\n",
    "  z_text = [[str(y) for y in x] for x in z]\n",
    "\n",
    "  fig_cm = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale='Viridis')\n",
    "\n",
    "  fig_cm.update_layout(\n",
    "      height=400, width=400,\n",
    "      showlegend=True,\n",
    "      margin={'t':150, 'l':0},\n",
    "      title={'text' : 'Matriz de Confusión', 'x':0.5, 'xanchor': 'center'},\n",
    "      xaxis = {'title_text':'Valor Real', 'tickangle':45, 'side':'top'},\n",
    "      yaxis = {'title_text':'Valor Predicho', 'tickmode':'linear'},\n",
    "  )\n",
    "  fig_cm.show()\n",
    "\n",
    "\n",
    "  # curva roc (definido para clasificación binaria)\n",
    "  fig_roc = None\n",
    "  if y_score is not None:\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=pos_label)\n",
    "    fig_roc = px.area(\n",
    "        x=fpr, y=tpr,\n",
    "        title = f'Curva ROC (AUC={auc(fpr, tpr):.4f})',\n",
    "        labels=dict(x='Ratio Falsos Positivos', y='Ratio Verdaderos Positivos'),\n",
    "        width=400, height=400\n",
    "    )\n",
    "    fig_roc.add_shape(type='line', line=dict(dash='dash'), x0=0, x1=1, y0=0, y1=1)\n",
    "\n",
    "    fig_roc.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig_roc.update_xaxes(constrain='domain')\n",
    "    \n",
    "    fig_roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7691fc-b486-4595-8815-3a762cf68a25",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6b090-fbb4-4a51-9602-c64084ede937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos el corpus de tweets y los labels correspondientes a \n",
    "# las tareas 1 de clasificación binaria y clasificación multilabel\n",
    "df_train            = pd.read_csv('https://raw.githubusercontent.com/carlossuazo/davincis-iberlef-2022/main/data/training_data/train_data.csv', header=None, names = [TEXT_COL])\n",
    "df_train_labels_st1 = pd.read_csv('https://raw.githubusercontent.com/carlossuazo/davincis-iberlef-2022/main/data/training_data/train_labels_subtask_1.csv', header=None, names = [CLASS_COL_ST1])\n",
    "df_train_labels_st2 = pd.read_csv('https://raw.githubusercontent.com/carlossuazo/davincis-iberlef-2022/main/data/training_data/train_labels_subtask_2.csv', header=None, names = CLASS_COL_ST2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c49dba-41db-4cf1-8317-8f1f4afaa0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train_lst1 = pd.concat([df_train, df_train_labels_st1], axis = 1)\n",
    "df_train_lst2 = pd.concat([df_train, df_train_labels_st2], axis = 1)\n",
    "display(df_train_lst1)\n",
    "display(df_train_lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6317fa8-ecf0-4ddd-8acd-fe3b0d38e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener algunas estadísticas sobre los datos\n",
    "categories = sorted(df_train_lst1[CLASS_COL_ST1].unique(), reverse=False)\n",
    "hist= Counter(df_train_lst1[CLASS_COL_ST1]) \n",
    "print(f'Total de instancias -> {df_train_lst1.shape[0]}')\n",
    "\n",
    "print(f'Categorías -> {categories}')\n",
    "print(f'Comentario de ejemplo -> {df_train_lst1[CLASS_COL_ST1][0]}')\n",
    "print(f'Categoría del comentario -> {df_train_lst1[CLASS_COL_ST1][0]}')\n",
    "\n",
    "fig = go.Figure(layout=go.Layout(height=400, width=600))\n",
    "fig.add_trace(go.Bar(x=categories, y=[hist[cat] for cat in categories]))\n",
    "fig.show()\n",
    "# Hacer entrenamiento con el corpus desbalanceado\n",
    "# Probar rellenando los datos de las categorías 0 y 1, cortar los datos de la categoría 2 utilizar la función iloc, traer 1052 de cada categoría \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897ae08-3c50-478a-9661-7e0a143773df",
   "metadata": {},
   "source": [
    "> No notamos un desbalance significativo en las clases, de momento intentaremos trabajar con estos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979756fb-3df5-4bc5-bdeb-251345106f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtener conjuntos de entrenamiento (90%) y validación (10%)\n",
    "seed = 0  # fijar random_state para reproducibilidad\n",
    "train, val = train_test_split(df_train_lst1, test_size=.1, stratify=df_train_lst1[CLASS_COL_ST1], random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3b89a-2001-400e-bebf-d0223a9a6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline():\n",
    "    return Pipeline([\n",
    "        ('dataVect', CountVectorizer(analyzer=spanish_stemmer)),\n",
    "        ('tfidf', TfidfTransformer(smooth_idf=True, use_idf=True)),\n",
    "     ])\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a3d91-4865-4a43-a765-fec5f93707e3",
   "metadata": {},
   "source": [
    "# Training SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db4aaf-ea39-4039-bcbd-766772ae8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CARLOS OSMAN SUAZO BACA #####\n",
    "# Aquí entrenaremos el modelo utilizando máquinas de vector soporte para la \n",
    "# clasificación.\n",
    "# crear el pipeline (solo incluyendo los pasos de pre-procesamiento)\n",
    "model = preprocess_pipeline()\n",
    "\n",
    "# crear el clasificador y añadirlo al model. Puede probar diferentes clasificadores\n",
    "classifier = SVC(probability=True)\n",
    "\n",
    "model.steps.append(('classifier', classifier))\n",
    "\n",
    "# entrenar el modelo\n",
    "model.fit(train[TEXT_COL], train[CLASS_COL_ST1])\n",
    "\n",
    "# guardar el modelo\n",
    "save_model(model, 'models/svc_model.pkl')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef92ca2-c98c-43e1-b9d4-38726d8853f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluación del modelo SVC\n",
    "Luego de entrenado el modelo, podemos evaluar su desempeño en los conjuntos de entrenamiento y validación.\n",
    "\n",
    "Ejecute la siguiente casilla para evaluar el modelo en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1802ae-33b6-48e8-9f4d-c6b7a401f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/svc_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff064af8-ebea-44db-a12f-1a0dbfae63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir y evaluar el modelo en el conjunto de entrenamiento\n",
    "print('==== Evaluación conjunto de entrenamiento ====')\n",
    "\n",
    "true_labels = train[CLASS_COL_ST1]\n",
    "\n",
    "m_pred = predict_model(model, train[TEXT_COL].to_list(), pref='m')\n",
    "\n",
    "# el nombre de los campos dependerá de pref al llamar a predic_model y las clases. Ver comentarios en la definición de la función\n",
    "evaluate_model(true_labels, m_pred['labels_m'], m_pred['scores_m_1'], '1')  \n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bd1b9-dbdd-4076-aa35-c3d46a934da1",
   "metadata": {},
   "source": [
    "Ejecutamos la siguiente casilla para evaluar el modelo en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2f803-f728-4704-b27f-6e12d548c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir y evaluar el modelo en el conjunto de validación\n",
    "print('==== Evaluación conjunto de validación ====')\n",
    "\n",
    "true_labels = val[CLASS_COL_ST1]\n",
    "\n",
    "m_pred = predict_model(model, val[TEXT_COL].to_list(), pref='m')\n",
    "\n",
    "# el nombre de los campos dependerá de pref al llamar a predic_model y las clases. Ver comentarios en la definición de la función\n",
    "evaluate_model(true_labels, m_pred['labels_m'], m_pred['scores_m_1'], '1')\n",
    "# desbalance.\n",
    "# falta de vopcabulario en las categoría neutral.\n",
    "# la intermedia es más dificil de clasificar.\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e0f23-a278-46ec-aef2-88710136482e",
   "metadata": {},
   "source": [
    "# Training SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb9ccb-86ae-4ac6-89bf-39c8b56a7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CARLOS OSMAN SUAZO BACA #####\n",
    "# Entrenamos el modelo y los almacenamos para poder hacer las validaciones\n",
    "# correspondientes. Intentaremos ver que clasificador tiene un mejor score en el \n",
    "# conjunto de entrenamiento (train) y el conjunto de validación(test o val).\n",
    "# crear el pipeline (solo incluyendo los pasos de pre-procesamiento)\n",
    "\n",
    "model = preprocess_pipeline()\n",
    "\n",
    "# crear el clasificador y añadirlo al model. Puede probar diferentes clasificadores\n",
    "classifier = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=1000, tol=None)\n",
    "\n",
    "model.steps.append(('classifier', classifier))\n",
    "\n",
    "# entrenar el modelo\n",
    "model.fit(train[TEXT_COL], train[CLASS_COL_ST1])\n",
    "\n",
    "# guardar el modelo\n",
    "save_model(model, 'models/sgd_classifier_model.pkl')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7da911-6894-43d7-b55f-e108c7abbea2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluación del modelo SGDClassifier\n",
    "Luego de entrenado el modelo, podemos evaluar su desempeño en los conjuntos de entrenamiento y validación.\n",
    "\n",
    "Ejecute la siguiente casilla para evaluar el modelo en el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16a6ea-3e74-489d-9371-ea7fc3cca7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/sgd_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6fd8da-bc00-4028-821a-3a41effd0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir y evaluar el modelo en el conjunto de entrenamiento\n",
    "\n",
    "print('==== Evaluación conjunto de entrenamiento ====')\n",
    "\n",
    "print(\"Precisión: \", model.score(train[TEXT_COL],train[CLASS_COL_ST1]))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fac89-aa94-4ca1-9dfa-03cb14a6df6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(train[TEXT_COL])\n",
    "report = classification_report(train[CLASS_COL_ST1], y_pred, output_dict = True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e9eb4-48b6-46c9-af6d-edc42665c660",
   "metadata": {},
   "source": [
    "Ejecutamos la siguiente casilla para evaluar el modelo en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52984a59-bc9d-45c6-b8c2-e5f85048115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir y evaluar el modelo en el conjunto de entrenamiento\n",
    "\n",
    "print('==== Evaluación conjunto de entrenamiento ====')\n",
    "\n",
    "print(\"Precisión: \", model.score(val[TEXT_COL],val[CLASS_COL_ST1]))\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975fadcb-6758-4751-8c9b-18442e122374",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(val[TEXT_COL])\n",
    "report = classification_report(val[CLASS_COL_ST1], y_pred, output_dict = True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c7553-8549-4efb-931c-bc74bc314afb",
   "metadata": {},
   "source": [
    "# Testing Some Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10993e60-8deb-4936-985f-d95979b7a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}  # diccionario para agrupar configuraciones y variables para su posterior uso\n",
    "\n",
    "# cargar el LabelEncoder\n",
    "#with open('models/label_encoder_reviews_test_0.78.pkl', 'rb') as f:\n",
    "cfg['label_encoder'] = load_model('models/label_encoder_reviews_test_0.78.pkl')\n",
    "\n",
    "# cargar TfidfVectorizer\n",
    "#with open('models/vectorizer_reviews_0.78.pkl', 'rb') as f:\n",
    "cfg['vectorizer'] =load_model('models/vectorizer_reviews_0.78.pkl')\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7b70c-2f9c-449e-be9b-ddf15673edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir y evaluar el modelo en el conjunto de validación\n",
    "print('==== Evaluación conjunto de validacióm ====')\n",
    "\n",
    "true_labels = val[CLASS_COL_ST1]\n",
    "\n",
    "m_pred = _predict_model(model, cfg, val[TEXT_COL].to_list(), pref='m')\n",
    "\n",
    "# el nombre de los campos dependerá de pref al llamar a predic_model y las clases. Ver comentarios en la definición de la función\n",
    "_evaluate_model(true_labels, m_pred['labels_m'])  \n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c195b9e4-65ab-466d-a7fb-e52554fc9c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogoal.ml import AutoML\n",
    "# from autogoal.datasets import haha\n",
    "# from autogoal.search import (\n",
    "#     PESearch,\n",
    "#     RichLogger,\n",
    "# )\n",
    "# from autogoal.kb import Seq, Sentence, VectorCategorical, Supervised\n",
    "# from autogoal.contrib import find_classes\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# import argparse\n",
    "\n",
    "# # parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument(\"--iterations\", type=int, default=10000)\n",
    "# # parser.add_argument(\"--timeout\", type=int, default=60)\n",
    "# # parser.add_argument(\"--memory\", type=int, default=2)\n",
    "# # parser.add_argument(\"--popsize\", type=int, default=50)\n",
    "# # parser.add_argument(\"--selection\", type=int, default=10)\n",
    "# # parser.add_argument(\"--global-timeout\", type=int, default=None)\n",
    "# # parser.add_argument(\"--examples\", type=int, default=None)\n",
    "# # parser.add_argument(\"--token\", default=None)\n",
    "# # parser.add_argument(\"--channel\", default=None)\n",
    "\n",
    "# # args = parser.parse_args()\n",
    "\n",
    "# # print(args)\n",
    "\n",
    "# for cls in find_classes():\n",
    "#     print(\"Using: %s\" % cls.__name__)\n",
    "\n",
    "# # Load dataset\n",
    "# X, y = df_train_lst1[TEXT_COL], df_train_lst1[CLASS_COL_ST1]\n",
    "\n",
    "\n",
    "# X_train = train[TEXT_COL].tolist()\n",
    "# y_train = train[CLASS_COL_ST1].to_numpy()\n",
    "\n",
    "# X_test = val[TEXT_COL].tolist()\n",
    "# y_test = val[CLASS_COL_ST1].to_numpy()\n",
    "\n",
    "# classifier = AutoML(\n",
    "#     search_algorithm=PESearch,\n",
    "#     input=(Seq[Sentence], Supervised[VectorCategorical]),\n",
    "#     output=VectorCategorical,\n",
    "#     search_iterations=10000,\n",
    "#     score_metric = f1_score,\n",
    "#     errors=\"warn\",\n",
    "#     pop_size=50,\n",
    "#     search_timeout= None,\n",
    "#     evaluation_timeout = 60,\n",
    "#     memory_limit= 2 * 1024 ** 3,\n",
    "#     registry=find_classes(\"Keras|Bert\"),\n",
    "# )\n",
    "\n",
    "# # loggers = [RichLogger()]\n",
    "\n",
    "# # if None:\n",
    "# #     from autogoal.contrib.telegram import TelegramLogger\n",
    "\n",
    "# #     telegram = TelegramLogger(token=None, name=f\"HAHA\", channel=None,)\n",
    "# #     loggers.append(telegram)\n",
    "\n",
    "# classifier.fit(X_train, y_train)\n",
    "# score = classifier.score(X_test, y_test)\n",
    "\n",
    "# print(score)\n",
    "# print(classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
